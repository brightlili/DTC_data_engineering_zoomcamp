
ğŸ§­ Workflow Orchestration

This module focuses on workflow orchestration in data engineering using tools like Apache Airflow. Youâ€™ll learn how to schedule, monitor, and manage data pipelines reliably in production environments.

â¸»

ğŸ› ï¸ Tech Stack
	â€¢	Orchestration Tool: Apache Airflow (via Docker)
	â€¢	Language: Python
	â€¢	Infrastructure: Docker Compose
	â€¢	Data Source: NYC Taxi Data
	â€¢	Database: PostgreSQL
	â€¢	Cloud: GCP (optional)

â¸»

ğŸ“¦ Folder Structure

workflow_orchestration/
â”‚
â”œâ”€â”€ dags/                    # Airflow DAGs (pipelines) live here
â”‚   â””â”€â”€ ingest_local.py
â”‚
â”œâ”€â”€ docker-compose.yaml      # Setup for Airflow and supporting services
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ airflow.cfg              # (if customized configuration)
â”‚
â””â”€â”€ README.md                # Documentation for the orchestration project


â¸»

ğŸš€ How to Get Started

1. Clone the repo & navigate to this folder:

git clone https://github.com/brightlili/DTC_data_engineering_zoomcamp
cd workflow_orchestration

2. Start Airflow using Docker Compose:

docker-compose up -d

Airflow UI should be available at: http://localhost:8080
Username: airflow
Password: airflow

â¸»

ğŸ Running Your DAG
	1.	Add your Python DAG file under the dags/ directory.
	2.	Make sure your DAG is picked up in the Airflow UI.
	3.	Enable and trigger it manually or set up a schedule.

â¸»

ğŸ”„ Common Commands

# To stop all containers
docker-compose down

# To check logs
docker-compose logs

# To access the Airflow webserver container
docker exec -it <webserver_container_id> bash


â¸»

ğŸ“š References
	â€¢	Airflow Docs
	â€¢	DataTalksClub Zoomcamp: Week 2