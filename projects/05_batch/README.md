
âš¡ Batch Processing with Spark

This module introduces Apache Spark as a scalable batch processing engine. Youâ€™ll use PySpark to read large datasets, transform them, and write the results back to storage (GCS or local).

â¸»

ðŸ§° Tech Stack

Tool	Purpose
Apache Spark	Distributed batch data processing
PySpark	Python interface to Spark
GCS / Local FS	Source and destination file systems
Docker	Run Spark locally using a containerized environment


â¸»

ðŸ“ Folder Structure

batch_processing/
â”‚
â”œâ”€â”€ data/                         # Source data (CSV or Parquet)
â”œâ”€â”€ notebooks/                    # Jupyter notebooks for testing
â”œâ”€â”€ spark_script.py               # Main PySpark script
â”œâ”€â”€ Dockerfile                    # Build PySpark Docker image
â”œâ”€â”€ requirements.txt              # Python dependencies (optional)
â””â”€â”€ README.md


â¸»

âš™ï¸ Setup Instructions

1. Build Docker Image

docker build -t spark-processing .

2. Run the Spark Job

Example (local file system):

docker run -it \
  -v $(pwd)/data:/opt/spark/data \
  spark-processing \
    spark-submit spark_script.py

Or, if youâ€™re reading from a GCS bucket, make sure to:
	â€¢	Mount the service account key file
	â€¢	Set GOOGLE_APPLICATION_CREDENTIALS inside the container

â¸»

ðŸ§ª Example PySpark Operations
	â€¢	Reading Parquet data:

df = spark.read.parquet("data/yellow_tripdata_2021-01.parquet")


	â€¢	Filtering and transforming:

df_filtered = df.filter(df.passenger_count > 0)
df_result = df_filtered.groupBy("PULocationID").count()


	â€¢	Writing results:

df_result.write.parquet("output/pickup_counts", mode="overwrite")



â¸»

ðŸ“š References
	â€¢	Apache Spark Docs
	â€¢	PySpark API Reference
	â€¢	DataTalksClub Zoomcamp â€“ Module 5
