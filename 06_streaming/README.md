
ğŸ“¡ Streaming Data with Kafka and Spark

This module introduces stream processing, where data is continuously ingested and processed in real-time. Youâ€™ll build a Kafka pipeline, produce and consume messages, and process them using Spark Structured Streaming.

â¸»

ğŸ¯ Learning Objectives
	â€¢	Understand real-time vs batch processing
	â€¢	Set up a Kafka message queue
	â€¢	Produce and consume messages using Python
	â€¢	Process streaming data using Spark
	â€¢	Write streaming outputs to storage (e.g., parquet, BigQuery)

â¸»

ğŸ› ï¸ Tools & Stack

Tool	Use Case
Kafka	Distributed event streaming platform
Spark Streaming	Real-time data transformation engine
Docker Compose	Container orchestration
Python + Faust (optional)	Lightweight stream processing
GCS or Local FS	Sink for processed stream data


â¸»

ğŸ“ Folder Structure

streaming/
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ docker-compose.yml         # Kafka + Zookeeper services
â”‚   â”œâ”€â”€ producer.py                # Sends data to Kafka topic
â”‚   â””â”€â”€ consumer.py                # Reads data from Kafka topic
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ spark_streaming.py         # Structured Streaming with Spark
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_data.json           # Sample input data
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


â¸»

âš™ï¸ Setup Instructions

1. Start Kafka with Docker Compose

cd kafka
docker-compose up

This spins up:
	â€¢	Kafka broker on port 9092
	â€¢	Zookeeper on port 2181

2. Produce messages

python producer.py

This script sends taxi ride data to Kafka topic rides.

3. Consume messages

python consumer.py

This script listens to rides topic and prints incoming messages.

â¸»

âš¡ Streaming with Spark

Run the Spark Streaming script (local or Dockerized):

spark-submit spark_streaming.py

This will:
	â€¢	Read from Kafka topic
	â€¢	Parse and transform JSON data
	â€¢	Write results to parquet files

â¸»

ğŸ”„ Real-World Use Case

Stream NYC taxi ride data â†’ Kafka â†’ Spark â†’ Transformed output in parquet files or BigQuery.

â¸»

ğŸ§ª Bonus: Optional Enhancements
	â€¢	Use Faust (Python-native stream processor)
	â€¢	Add schema registry with Kafka
	â€¢	Persist to BigQuery or PostgreSQL
	â€¢	Add monitoring with Prometheus + Grafana

â¸»

ğŸ“š References
	â€¢	Apache Kafka
	â€¢	Spark Structured Streaming
	â€¢	Kafka + Python Guide
	â€¢	Zoomcamp Streaming Module
